{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from git import Repo\n",
    "from langchain.document_loaders.generic import GenericLoader  # this package is for code base\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone Github Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\Generative_AI\\\\Projects\\\\Source-Code-Analysis-using-GenAI\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.repo.base.Repo 'f:\\\\Generative_AI\\\\Projects\\\\Source-Code-Analysis-using-GenAI\\\\research\\\\test_repo\\\\.git'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path= \"test_repo/\"\n",
    "\n",
    "Repo.clone_from(\"https://github.com/Manirathinam21/End-to-End-ML-Project-with-MLFlow.git\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "loader = GenericLoader.from_filesystem(repo_path+'/src/mlProject',\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='import os\\nimport sys\\nimport logging\\n\\nlogging_str = \"[%(asctime)s: %(levelname)s: %(module)s: %(message)s]\"\\n\\nlog_dir = \"logs\"\\nlog_filepath = os.path.join(log_dir,\"running_logs.log\")\\nos.makedirs(log_dir, exist_ok=True)\\n\\n\\nlogging.basicConfig(\\n    level= logging.INFO,\\n    format= logging_str,\\n\\n    handlers=[\\n        logging.FileHandler(log_filepath),\\n        logging.StreamHandler(sys.stdout)\\n    ]\\n)\\n\\nlogger = logging.getLogger(\"mlProjectLogger\")', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom pathlib import Path\\nimport urllib.request as request\\nimport zipfile\\nfrom mlProject import logger\\nfrom mlProject.utils.common import get_size\\nfrom mlProject.entity.config_entity import (DataIngestionConfig)\\n\\nclass DataIngestion:\\n    def __init__(self, config: DataIngestionConfig):\\n        self.config= config\\n\\n    def download_file(self):\\n        if not os.path.exists(self.config.local_data_file):\\n            filename, headers = request.urlretrieve(\\n                url = self.config.source_URL,\\n                filename = self.config.local_data_file\\n            )\\n            logger.info(f\"{filename} download! with following info: \\\\n{headers}\")\\n        else:\\n            logger.info(f\"File already exists of size: {get_size(Path(self.config.local_data_file))}\")\\n\\n    def extract_zip_file(self):\\n        \"\"\"\\n        zip_file_path: str\\n        Extracts the zip file into the data directory\\n        Function returns None\\n        \"\"\"\\n        unzip_path = self.config.unzip_dir\\n        os.makedirs(unzip_path, exist_ok=True)\\n        with zipfile.ZipFile(self.config.local_data_file, \\'r\\') as zip_ref:\\n            zip_ref.extractall(unzip_path)\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom mlProject import logger\\nfrom mlProject.entity.config_entity import DataTransformationConfig\\n\\nclass DataTransformation:\\n    def __init__ (self, config: DataTransformationConfig):\\n        self.config= config\\n\\n    \"\"\"Note: you can add different data transformation techniques such as scaler, PCA and all kinds of\\n     EDA in ML cycle here before passing this data to the model\\n     \\n    I\\'m only adding train_test_splitting becoz this data is already cleaned up\"\"\"\\n\\n    def train_test_splitting(self):\\n        data= pd.read_csv(self.config.data_path)\\n\\n        # split the data into training and test sets (0.70 , 0.30) split.\\n        train, test= train_test_split(data, test_size=0.30)\\n\\n        train.to_csv(os.path.join(self.config.root_dir, \"train.csv\"), index=False)\\n        test.to_csv(os.path.join(self.config.root_dir, \"test.csv\"), index=False)\\n\\n        logger.info(\"splited data into train and test sets\")\\n        logger.info(train.shape)\\n        logger.info(test.shape)\\n\\n        print(train.shape)\\n        print(test.shape)', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport pandas as pd\\nfrom mlProject import logger\\nfrom mlProject.entity.config_entity import DataValidationConfig \\n\\n\\nclass DataValiadtion:\\n    def __init__(self, config: DataValidationConfig):\\n        self.config = config\\n\\n\\n    def validate_all_columns(self)-> bool:\\n        try:\\n            validation_status = None\\n\\n            data = pd.read_csv(self.config.unzip_data_dir)\\n            all_cols = list(data.columns)\\n\\n            all_schema = self.config.all_schema.keys()\\n\\n            for col in all_cols:\\n                if col not in all_schema:\\n                    validation_status = False\\n                    with open(self.config.STATUS_FILE, \\'w\\') as f:\\n                        f.write(f\"Validation status: {validation_status}\")\\n                else:\\n                    validation_status = True\\n                    with open(self.config.STATUS_FILE, \\'w\\') as f:\\n                        f.write(f\"Validation status: {validation_status}\")\\n            return validation_status\\n        \\n        except Exception as e:\\n            raise e', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\data_validation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport pandas as pd\\nimport numpy as np\\nimport json\\nimport joblib\\nimport mlflow \\nimport mlflow.sklearn\\nfrom pathlib import Path\\nfrom urllib.parse import urlparse\\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\\nfrom mlProject.entity.config_entity import ModelEvaluationConfig\\nfrom mlProject.utils.common import save_json\\nfrom mlProject import logger\\n\\n\\n\\n\\nclass ModelEvaluation:\\n    def __init__ (self, config:ModelEvaluationConfig ):\\n        self.config= config\\n\\n    def eval_metrics(self, actual, pred):\\n        rmse= np.sqrt(mean_squared_error(actual, pred))\\n        mae= mean_absolute_error(actual, pred)\\n        r2= r2_score(actual, pred)\\n        return rmse, mae, r2\\n\\n    def log_into_mlflow(self):\\n        test_data= pd.read_csv(self.config.test_data_path)\\n        model= joblib.load(self.config.model_path)\\n\\n        test_x= test_data.drop([self.config.target_column], axis=1)\\n        test_y= test_data[[self.config.target_column]]\\n\\n        mlflow.set_registry_uri(self.config.mlflow_uri)\\n        tracking_url_type_store= urlparse(mlflow.get_tracking_uri()).scheme\\n\\n        with mlflow.start_run():\\n\\n            pred_y= model.predict(test_x)\\n            (rmse, mae, r2)= self.eval_metrics(test_y, pred_y)\\n\\n            # Saving metrics as local\\n            scores = {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\\n            save_json(path= Path(self.config.metric_file_name), data=scores)\\n            \\n            \\n            mlflow.log_params(self.config.all_params)\\n\\n            mlflow.log_metric(\"rmse\", rmse)\\n            mlflow.log_metric(\"mae\", mae)\\n            mlflow.log_metric(\"r2\", r2)\\n\\n            # Model registry does not work with file store\\n            if tracking_url_type_store != \"file\":\\n                # Register the model\\n                # There are other ways to use the Model Registry, which depends on the use case,\\n                # please refer to the doc for more information:\\n                # https://mlflow.org/docs/latest/model-registry.html#api-workflow\\n                mlflow.sklearn.log_model(model, \"model\", registered_model_name=\"ElasticnetModel\")\\n            else:\\n                mlflow.sklearn.log_model(model, \"model\")\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport joblib\\nimport pandas as pd\\nfrom sklearn.linear_model import ElasticNet\\nfrom mlProject.entity.config_entity import ModelTrainerConfig\\nfrom mlProject import logger\\n\\n\\nclass ModelTrainer:\\n    def __init__ (self, config: ModelTrainerConfig):\\n        self.config = config\\n\\n    def train(self):\\n        train_data= pd.read_csv(self.config.train_data_path)\\n        test_data= pd.read_csv(self.config.test_data_path)\\n\\n        train_x= train_data.drop([self.config.target_column], axis=1) \\n        test_x= test_data.drop([self.config.target_column], axis=1)\\n        train_y= train_data[[self.config.target_column]]\\n        test_y= test_data[[self.config.target_column]]\\n\\n        lr= ElasticNet(l1_ratio= self.config.l1_ratio, alpha= self.config.alpha, random_state=42)\\n        lr.fit(train_x, train_y)\\n\\n        joblib.dump(lr, os.path.join(self.config.root_dir, self.config.model_name))', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\components\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject.constants import *\\nfrom mlProject.utils.common import read_yaml, create_directories\\nfrom mlProject.entity.config_entity import (DataIngestionConfig, DataValidationConfig, \\n                            DataTransformationConfig, ModelTrainerConfig, ModelEvaluationConfig)\\n\\nclass ConfigurationManager:\\n    def __init__(self,\\n        config_filepath = CONFIG_FILE_PATH,\\n        params_filepath = PARAMS_FILE_PATH,\\n        schema_filepath = SCHEMA_FILE_PATH):\\n\\n        self.config = read_yaml(config_filepath)\\n        self.params = read_yaml(params_filepath)\\n        self.schema = read_yaml(schema_filepath)\\n\\n        create_directories([self.config.artifacts_root])\\n\\n    def get_data_ingestion_config(self) -> DataIngestionConfig:\\n        config= self.config.data_ingestion\\n\\n        create_directories([config.root_dir])\\n\\n        data_ingestion_config= DataIngestionConfig(\\n            root_dir = config.root_dir,\\n            source_URL = config.source_URL,\\n            local_data_file = config.local_data_file,\\n            unzip_dir = config.unzip_dir \\n        )\\n        return data_ingestion_config\\n    \\n    def get_data_validation_config(self) -> DataValidationConfig:\\n        config = self.config.data_validation\\n        schema = self.schema.COLUMNS\\n\\n        create_directories([config.root_dir])\\n\\n        data_validation_config = DataValidationConfig(\\n            root_dir=config.root_dir,\\n            STATUS_FILE=config.STATUS_FILE,\\n            unzip_data_dir = config.unzip_data_dir,\\n            all_schema= schema,\\n        )\\n        return data_validation_config\\n\\n\\n    def get_data_transformation_config(self) -> DataTransformationConfig:\\n        config= self.config.data_transformation\\n\\n        create_directories([config.root_dir])\\n\\n        data_transformation_config= DataTransformationConfig(\\n            root_dir= config.root_dir,\\n            data_path= config.data_path)\\n        return data_transformation_config\\n\\n\\n    def get_model_trainer_config(self) -> ModelTrainerConfig:\\n        config= self.config.model_trainer\\n        schema= self.schema.TARGET_COLUMN\\n        params= self.params.ElasticNet\\n\\n        create_directories([config.root_dir])\\n\\n        model_trainer_config= ModelTrainerConfig(\\n            root_dir= config.root_dir,\\n            train_data_path= config.train_data_path,\\n            test_data_path= config.test_data_path,\\n            model_name= config.model_name,\\n            l1_ratio= params.l1_ratio,\\n            alpha= params.alpha,\\n            target_column= schema.name\\n        )\\n        return model_trainer_config\\n\\n\\n    def get_model_evaluation_config(self) ->ModelEvaluationConfig:\\n\\n        config= self.config.model_evaluation\\n        params= self.params.ElasticNet\\n        schema= self.schema.TARGET_COLUMN\\n\\n        create_directories([config.root_dir])\\n\\n        model_evaluation_config= ModelEvaluationConfig(\\n            root_dir=  config.root_dir,\\n            test_data_path= config.test_data_path,\\n            model_path= config.model_path,\\n            metric_file_name = config.metric_file_name,\\n            target_column = schema.name,\\n            all_params= params,\\n            mlflow_uri= \"https://dagshub.com/manirathinam21/End-to-End-ML-Project-with-MLFlow.mlflow\")\\n        return model_evaluation_config', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\config\\\\configuration.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\config\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from pathlib import Path\\nfrom pathlib import Path\\n\\nCONFIG_FILE_PATH = Path(\"config/config.yaml\")\\nPARAMS_FILE_PATH = Path(\"params.yaml\")\\nSCHEMA_FILE_PATH = Path(\"schema.yaml\")', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\constants\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from pathlib import Path\\nfrom dataclasses import dataclass\\n\\n@dataclass(frozen=True)\\nclass DataIngestionConfig:\\n    root_dir: Path \\n    source_URL: str\\n    local_data_file: Path\\n    unzip_dir: Path\\n\\n@dataclass(frozen=True)\\nclass DataValidationConfig:\\n    root_dir: Path\\n    STATUS_FILE: str\\n    unzip_data_dir: Path\\n    all_schema: dict\\n\\n@dataclass(frozen=True)\\nclass DataTransformationConfig:\\n    root_dir: Path\\n    data_path: Path \\n\\n@dataclass(frozen=True)\\nclass ModelTrainerConfig:\\n    root_dir: Path\\n    train_data_path: Path\\n    test_data_path: Path\\n    model_name: str\\n    l1_ratio: float\\n    alpha: float\\n    target_column: str\\n\\n@dataclass(frozen=True)\\nclass ModelEvaluationConfig:\\n    root_dir: Path\\n    test_data_path: Path\\n    model_path: Path\\n    metric_file_name: Path\\n    target_column: str\\n    mlflow_uri: str\\n    all_params: dict', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\entity\\\\config_entity.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\entity\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"import pandas as pd\\nimport numpy as np\\nimport joblib\\nfrom pathlib import Path\\n\\n\\nclass PredictionPipeline:\\n    def __init__(self):\\n        self.model=joblib.load(Path('model/model.joblib'))\\n\\n    def predict(self, data):\\n        prediction=self.model.predict(data)\\n        return prediction\", metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\prediction.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.data_ingestion import DataIngestion\\nfrom mlProject import logger\\n\\nSTAGE_NAME = \"Data Ingestion Stage\" \\n\\n\\nclass DataIngestionTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config= ConfigurationManager()\\n        data_ingestion_config = config.get_data_ingestion_config()\\n        data_ingestion = DataIngestion(config= data_ingestion_config)\\n        data_ingestion.download_file()\\n        data_ingestion.extract_zip_file()\\n\\n\\nif __name__==\"__main__\":\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj=DataIngestionTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx===========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e\\n', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\stage_01_data_ingestion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject import logger\\nfrom mlProject.components.data_validation import DataValiadtion\\nfrom mlProject.config.configuration import ConfigurationManager\\n\\n\\n\\nclass DataValidationTrainingPipeline:\\n    def __init__(self):\\n        pass\\n    \\n    def main(self):\\n        config = ConfigurationManager()\\n        data_validation_config = config.get_data_validation_config()\\n        data_validation =DataValiadtion(config= data_validation_config)\\n        data_validation.validate_all_columns()\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\\n        obj = DataValidationTrainingPipeline()\\n        obj.main()\\n        logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\")\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\stage_02_data_validation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject import logger\\nfrom pathlib import Path\\nfrom mlProject.components.data_transformation import DataTransformation\\nfrom mlProject.config.configuration import ConfigurationManager\\n\\nSTAGE_NAME = \\'Data Transformation Stage\\'\\n\\nclass DataTransformationTrainingPipeline:\\n    def __init__ (self):\\n        pass\\n\\n    def main(self):\\n        try:\\n            with open(Path(\"artifacts/data_validation/status.txt\"), \\'r\\') as f:\\n                status = f.read().split(\" \")[-1] \\n\\n            if status == \"True\":\\n                config= ConfigurationManager()\\n                data_transformation_config = config.get_data_transformation_config()\\n                data_transformation = DataTransformation(config= data_transformation_config)\\n                data_transformation.train_test_splitting()\\n            else:\\n                raise Exception(\"your data schema is not valid\")\\n                \\n        except Exception as e:\\n            print(e)\\n\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        logger.info(f\\'>>>>>> Stage {STAGE_NAME} started <<<<<<\\')\\n        obj=DataTransformationTrainingPipeline()\\n        obj.main()\\n        logger.info(f\\'>>>>>> Stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\\')\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\stage_03_data_transformation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject import logger\\nfrom mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.model_trainer import ModelTrainer\\n\\nSTAGE_NAME= \\'Model Trainer Stage\\'\\n\\nclass ModelTrainerTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config= ConfigurationManager()\\n        model_trainer_config= config.get_model_trainer_config()\\n        model_trainer= ModelTrainer(config= model_trainer_config)\\n        model_trainer.train()\\n\\n\\nif __name__ == \"__main__\":\\n    try:\\n        logger.info(f\\'>>>>>> Stage {STAGE_NAME} started <<<<<<\\')\\n        obj=ModelTrainerTrainingPipeline()\\n        obj.main()\\n        logger.info(f\\'>>>>>> Stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\\')\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\stage_04_model_trainer.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from mlProject import logger\\nfrom mlProject.config.configuration import ConfigurationManager\\nfrom mlProject.components.model_evaluation import ModelEvaluation\\n\\n\\nSTAGE_NAME= \\'Model Evaluation Stage\\'\\n\\nclass ModelEvaluationTrainingPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        config= ConfigurationManager()\\n        model_evaluation_config= config.get_model_evaluation_config()\\n        model_evaluation= ModelEvaluation(config= model_evaluation_config)\\n        model_evaluation.log_into_mlflow()\\n\\n\\nif __name__ == \"__main__\":\\n    try:\\n        logger.info(f\\'>>>>>> Stage {STAGE_NAME} started <<<<<<\\')\\n        obj=ModelEvaluationTrainingPipeline()\\n        obj.main()\\n        logger.info(f\\'>>>>>> Stage {STAGE_NAME} completed <<<<<<\\\\n\\\\nx==========x\\')\\n    except Exception as e:\\n        logger.exception(e)\\n        raise e', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\stage_05_model_evaluation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\pipeline\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom box.exceptions import BoxValueError\\nimport yaml\\nfrom mlProject import logger\\nimport json\\nimport joblib\\nfrom ensure import ensure_annotations\\nfrom box import ConfigBox\\nfrom pathlib import Path\\nfrom typing import Any\\n\\n\\n\\n@ensure_annotations\\ndef read_yaml(path_to_yaml: Path) -> ConfigBox:\\n    \"\"\"reads yaml file and returns\\n\\n    Args:\\n        path_to_yaml (str): path like input\\n\\n    Raises:\\n        ValueError: if yaml file is empty\\n        e: empty file\\n\\n    Returns:\\n        ConfigBox: ConfigBox type\\n    \"\"\"\\n    try:\\n        with open(path_to_yaml) as yaml_file:\\n            content = yaml.safe_load(yaml_file)\\n            logger.info(f\"yaml file: {path_to_yaml} loaded successfully\")\\n            return ConfigBox(content)\\n    except BoxValueError:\\n        raise ValueError(\"yaml file is empty\")\\n    except Exception as e:\\n        raise e\\n    \\n\\n\\n@ensure_annotations\\ndef create_directories(path_to_directories: list, verbose=True):\\n    \"\"\"create list of directories\\n\\n    Args:\\n        path_to_directories (list): list of path of directories\\n        ignore_log (bool, optional): ignore if multiple dirs is to be created. Defaults to False.\\n    \"\"\"\\n    for path in path_to_directories:\\n        os.makedirs(path, exist_ok=True)\\n        if verbose:\\n            logger.info(f\"created directory at: {path}\")\\n\\n\\n@ensure_annotations\\ndef save_json(path: Path, data: dict):\\n    \"\"\"save json data\\n\\n    Args:\\n        path (Path): path to json file\\n        data (dict): data to be saved in json file\\n    \"\"\"\\n    with open(path, \"w\") as f:\\n        json.dump(data, f, indent=4)\\n\\n    logger.info(f\"json file saved at: {path}\")\\n\\n\\n\\n\\n@ensure_annotations\\ndef load_json(path: Path) -> ConfigBox:\\n    \"\"\"load json files data\\n\\n    Args:\\n        path (Path): path to json file\\n\\n    Returns:\\n        ConfigBox: data as class attributes instead of dict\\n    \"\"\"\\n    with open(path) as f:\\n        content = json.load(f)\\n\\n    logger.info(f\"json file loaded succesfully from: {path}\")\\n    return ConfigBox(content)\\n\\n\\n@ensure_annotations\\ndef save_bin(data: Any, path: Path):\\n    \"\"\"save binary file\\n\\n    Args:\\n        data (Any): data to be saved as binary\\n        path (Path): path to binary file\\n    \"\"\"\\n    joblib.dump(value=data, filename=path)\\n    logger.info(f\"binary file saved at: {path}\")\\n\\n\\n@ensure_annotations\\ndef load_bin(path: Path) -> Any:\\n    \"\"\"load binary data\\n\\n    Args:\\n        path (Path): path to binary file\\n\\n    Returns:\\n        Any: object stored in the file\\n    \"\"\"\\n    data = joblib.load(path)\\n    logger.info(f\"binary file loaded from: {path}\")\\n    return data\\n\\n\\n\\n@ensure_annotations\\ndef get_size(path: Path) -> str:\\n    \"\"\"get size in KB\\n\\n    Args:\\n        path (Path): path of the file\\n\\n    Returns:\\n        str: size in KB\\n    \"\"\"\\n    size_in_kb = round(os.path.getsize(path)/1024)\\n    return f\"~ {size_in_kb} KB\"', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\utils\\\\common.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\mlProject\\\\utils\\\\__init__.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunkings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter= RecursiveCharacterTextSplitter.from_language(language= Language.PYTHON,\n",
    "                               chunk_size =2000,\n",
    "                               chunk_overlap= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks= documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
